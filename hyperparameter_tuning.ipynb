{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2-XLsLHoPXYv"},"outputs":[],"source":["%%bash\n","\n","apt-get install swig\n","\n","git clone https://github.com/pybox2d/pybox2d\n","cd pybox2d\n","python setup.py build\n","python setup.py install\n","\n","apt-get install -y xvfb\n","\n","# pip install \\\n","#     gym==0.21 \\\n","#     gym[box2d]==0.21 \\\n","#     pytorch-lightning==1.6.0 \\\n","#     optuna==2.7.0 \\\n","#     pyglet==1.5.27 \\\n","#     pyvirtualdisplay"]},{"cell_type":"code","source":["!pip install gym==0.21\n","# !pip install gym[box2d]==0.21\n","!pip install pytorch-lightning==1.6.0\n","!pip install optuna==2.7.0\n","!pip install pyglet==1.5.27\n","!pip install pyvirtualdisplay"],"metadata":{"id":"O8DjMBrgGN_s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install gym[box2d]"],"metadata":{"id":"HxcqgPCYG_2D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install pyvirtualdisplay"],"metadata":{"id":"rjmbXDagGERU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"PeLWIvgMLlU6"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"4qiN_TlIL0rs"}},{"cell_type":"markdown","metadata":{"id":"ZOSJl-X7zvs4"},"source":["#### Setup virtual display"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"FllWJVrvPhzz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702018059454,"user_tz":-120,"elapsed":4,"user":{"displayName":"אור מזרחי","userId":"14772102086814831247"}},"outputId":"b2889e66-f58c-4df5-a3fc-7353ce20d389"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyvirtualdisplay.display.Display at 0x79c7821dc850>"]},"metadata":{},"execution_count":52}],"source":["from pyvirtualdisplay import Display\n","Display(visible=False, size=(1400, 900)).start()"]},{"cell_type":"markdown","metadata":{"id":"Cz8DLleGz_TF"},"source":["#### Import the necessary code libraries"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"iVXUA1JoPmVM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702018059756,"user_tz":-120,"elapsed":6,"user":{"displayName":"אור מזרחי","userId":"14772102086814831247"}},"outputId":"82b0ca2b-d3fa-4f34-a708-a4f3ddd447ab"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}],"source":["import copy\n","import statistics\n","import gym\n","import torch\n","import optuna\n","\n","import numpy as np\n","import torch.nn.functional as F\n","\n","from collections import deque, namedtuple\n","from IPython.display import HTML\n","from base64 import b64encode\n","\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torch.utils.data.dataset import IterableDataset\n","from torch.optim import AdamW\n","\n","from pytorch_lightning import LightningModule, Trainer\n","\n","from gym.wrappers import RecordVideo, RecordEpisodeStatistics\n","\n","from optuna.integration import PyTorchLightningPruningCallback\n","\n","device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","num_gpus = torch.cuda.device_count()"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"XPGaTA9mPmXn","executionInfo":{"status":"ok","timestamp":1702018059756,"user_tz":-120,"elapsed":5,"user":{"displayName":"אור מזרחי","userId":"14772102086814831247"}}},"outputs":[],"source":["def display_video(episode=0):\n","  video_file = open(f'/content/videos/rl-video-episode-{episode}.mp4', \"r+b\").read()\n","  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n","  return HTML(f\"<video width=600 controls><source src='{video_url}'></video>\")"]},{"cell_type":"markdown","metadata":{"id":"eLH52SgC0RRI"},"source":["#### Create the Deep Q-Network"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"BL3UviKCPmZ6","executionInfo":{"status":"ok","timestamp":1702018059756,"user_tz":-120,"elapsed":5,"user":{"displayName":"אור מזרחי","userId":"14772102086814831247"}}},"outputs":[],"source":["class DQN(nn.Module):\n","  \"\"\"\n","  The Deep Q-network\n","\n","  return a vector of Q-values.\n","  each Q-value for each one of the action that we can take.\n","  The Q-value is the expected cumelative return, given state and action.\n","  Q(s,a)\n","  \"\"\"\n","    def __init__(self, hidden_size, obs_size, n_actions):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            # Input: observation size (the state)\n","            nn.Linear(obs_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.ReLU(),\n","            # Output: Q-values (one Q-value for each action.)\n","            nn.Linear(hidden_size, n_actions),\n","        )\n","\n","    def forward(self, x):\n","      \"\"\"\n","      What should happen when we pass an observation to this nn?\n","      \"\"\"\n","      # apply the layers in the ANN to the input.\n","      # the Output: Q-values (one Q-value for each action.)\n","      return self.net(x.float())"]},{"cell_type":"markdown","metadata":{"id":"bnk0wSWj0hAz"},"source":["#### Create the policy"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"n0DzHUT_PmcT","executionInfo":{"status":"ok","timestamp":1702018059756,"user_tz":-120,"elapsed":5,"user":{"displayName":"אור מזרחי","userId":"14772102086814831247"}}},"outputs":[],"source":["def epsilon_greedy(state, env, net, epsilon=0.0):\n","  \"\"\"\n","  The epcilon greedy policy.\n","  Care to balance between exploration and explotation by epsilon.\n","  \"\"\"\n","  # Exploration >>\n","  if np.random.random() < epsilon:\n","      action = env.action_space.sample() # take random action.\n","\n","  # Explotation >>\n","  # choose the action that yield the higher q-value.\n","  else:\n","      state = torch.tensor([state]).to(device) # take the state\n","      q_values = net(state) # predicr the best Q-values for that state\n","      _, action = torch.max(q_values, dim=1) # Take the max action that yield the higer Q-value\n","      action = int(action.item()) # take the index of the best action\n","\n","  # return the action\n","  return action"]},{"cell_type":"markdown","metadata":{"id":"brJmKGkl0jge"},"source":["#### Create the replay buffer\n","> __implement the functionality that will allow us to turn our environment into a dataset.__\n","\n","- The replay buffer is a crucial component in training Deep Q-Networks (DQN) and other off-policy reinforcement learning algorithms.\n","- The replay buffer stores experiences (state transitions) observed by the __agent during interactions with the environment__ and allows for randomly sampling batches of experiences during the training process."]},{"cell_type":"code","execution_count":57,"metadata":{"id":"9T-fsyriPme6","executionInfo":{"status":"ok","timestamp":1702018060331,"user_tz":-120,"elapsed":21,"user":{"displayName":"אור מזרחי","userId":"14772102086814831247"}}},"outputs":[],"source":["# Define the names of each element in the tuple.\n","Experience = namedtuple(\n","    \"Experience\",\n","    field_names=[\"state\", \"action\", \"reward\", \"done\", \"new_state\"],\n",")"]},{"cell_type":"code","execution_count":58,"metadata":{"id":"nLprQhrtPmhR","executionInfo":{"status":"ok","timestamp":1702018060331,"user_tz":-120,"elapsed":19,"user":{"displayName":"אור מזרחי","userId":"14772102086814831247"}}},"outputs":[],"source":["class ReplayBuffer:\n","  \"\"\"\n","  Replay Buffer\n","\n","  Parameters:\n","  ----------\n","  - `capacity`: maximum number of experiences that the buffer can store.\n","  \"\"\"\n","\n","  def __init__(self, capacity):\n","    \"\"\"\n","    Initialization the replay buffer database\n","    \"\"\"\n","    self.buffer = deque(maxlen=capacity)\n","\n","  def __len__(self):\n","    \"\"\"\n","    return the length of the dataset\n","    \"\"\"\n","    return len(self.buffer)\n","\n","  def append(self, experience):\n","    \"\"\"\n","    add a new experience to the dataset\n","    \"\"\"\n","    self.buffer.append(experience)\n","\n","  def sample(self, batch_size):\n","    \"\"\"\n","    randomly samples a batch of experiences from the replay buffer\n","\n","    Random sampling helps in decorrelating the training data and\n","    breaking the temporal correlations present in consecutive experiences.\n","    The sampled batch is then typically used for updating the\n","    Q-network during the training process.\n","\n","    Parameters:\n","    ----------\n","    - `batch_size` determines the number of experiences to sample\n","    \"\"\"\n","    # choise `batch_size` indexes from the entire dataset\n","    indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n","    # extract a tensor of each component:\n","    states, actions, rewards, dones, next_states = zip(*(self.buffer[idx] for idx in indices))\n","\n","    #  returns a tuple arrays corresponding to the sampled batch of experiences.\n","    return (\n","          np.array(states),\n","          np.array(actions),\n","          np.array(rewards, dtype=np.float32),\n","          np.array(dones, dtype=np.bool),\n","          np.array(next_states),\n","        )"]},{"cell_type":"code","execution_count":59,"metadata":{"id":"_BQjkQOlPmkU","executionInfo":{"status":"ok","timestamp":1702018060332,"user_tz":-120,"elapsed":19,"user":{"displayName":"אור מזרחי","userId":"14772102086814831247"}}},"outputs":[],"source":["class RLDataset(IterableDataset):\n","  \"\"\"\n","  an implementation of a PyTorch IterableDataset designed for creating\n","  an iterable dataset from the `ReplayBuffer` class.\n","\n","  suitable for use in training reinforcement learning models.\n","\n","  Parameters:\n","  -----------\n","  - `buffer`: the dataset (that contain all the experience)\n","  - `sample_size`: the number of experiences to be sampled from the replay buffer in each iteration\n","  \"\"\"\n","\n","  def __init__(self, buffer, sample_size=200):\n","    self.buffer = buffer # set the dataset\n","    self.sample_size = sample_size # set sample size\n","\n","  def __iter__(self):\n","    \"\"\"\n","    Iter method.\n","    \"\"\"\n","    # extract `sample_size` of states, action, ...\n","    states, actions, rewards, dones, new_states = self.buffer.sample(self.sample_size)\n","\n","    # return the samples iteratively.\n","    for i in range(len(dones)):\n","      # ..return each experience..\n","      yield states[i], actions[i], rewards[i], dones[i], new_states[i]"]},{"cell_type":"markdown","metadata":{"id":"0yvDC9qF0oPr"},"source":["#### Create the environment"]},{"cell_type":"code","execution_count":60,"metadata":{"id":"6KlHL-kznY3C","executionInfo":{"status":"ok","timestamp":1702018060332,"user_tz":-120,"elapsed":19,"user":{"displayName":"אור מזרחי","userId":"14772102086814831247"}}},"outputs":[],"source":["def create_environment(name):\n","  \"\"\"\n","  Function for creating and configuring a gym environment.\n","\n","  Parameters:\n","    - name: String, the name of the gym environment.\n","\n","  Returns:\n","    A configured gym environment.\n","\n","  \"\"\"\n","\n","  # Create the gym environment with the specified name.\n","  env = gym.make(name)\n","\n","  # Wrap the environment with RecordVideo to record videos during training.\n","  env = RecordVideo(env, video_folder='./videos', episode_trigger=lambda x: x % 50 == 0)\n","\n","  # Wrap the environment with RecordEpisodeStatistics to record episode statistics.\n","  env = RecordEpisodeStatistics(env)\n","\n","  # Return the configured environment.\n","  return env\n"]},{"cell_type":"markdown","metadata":{"id":"WLmsqFuW1kkC"},"source":["#### Create the test/sampling function"]},{"cell_type":"code","execution_count":61,"metadata":{"id":"hwnhC_zCwKDm","executionInfo":{"status":"ok","timestamp":1702018060332,"user_tz":-120,"elapsed":19,"user":{"displayName":"אור מזרחי","userId":"14772102086814831247"}}},"outputs":[],"source":["  @torch.no_grad()\n","  def play_episode(env, q_net, buffer, policy=None, epsilon=0.):\n","    \"\"\"\n","    Function that play one epicode, and push the transitions\n","    (the experience) to the buffer (the memory)\n","    \"\"\"\n","    obs = env.reset() # get initial state\n","    done = False\n","\n","    while not done:\n","      if policy: # if there is a policy, act based on the policy\n","        action = policy(obs, env, q_net, epsilon=epsilon)\n","      else: # if there is no policy, act based on random policy\n","        action = env.action_space.sample()\n","\n","      # perform the action, get the transition\n","      next_obs, reward, done, info = env.step(action)\n","      # create Experience instance (contain the transition)\n","      exp = Experience(obs, action, reward, done, next_obs)\n","      # append the experience in the buffer mamory\n","      buffer.append(exp)\n","      # update the observation\n","      obs = next_obs"]},{"cell_type":"markdown","metadata":{"id":"sgXi6A4Z1p75"},"source":["#### Create the Deep Q-Learning algorithm"]},{"cell_type":"code","execution_count":80,"metadata":{"id":"tOmxUJ1vnY5d","executionInfo":{"status":"ok","timestamp":1702028911923,"user_tz":-120,"elapsed":1,"user":{"displayName":"אור מזרחי","userId":"14772102086814831247"}}},"outputs":[],"source":["class DeepQLearning(LightningModule):\n","  \"\"\"\n","  Implementing the Deep Q-learning algorithm.\n","\n","  Parameters\n","  -----------\n","  - `env_name`: The name of the gym environment that the agent will interact with.\n","  - `policy`: The exploration-exploitation policy function. Default is epsilon_greedy, which uses epsilon-greedy exploration.\n","  - `sample_fn`: The function used for collecting experiences. Default is play_episode, which plays episodes and adds experiences to the replay buffer.\n","  - `capacity`: The capacity of the replay buffer, determining how many experiences it can store.\n","  - `batch_size`: The size of batches sampled from the replay buffer during training.\n","  - `lr`: The learning rate for the optimizer.\n","  - `hidden_size`: The size of the hidden layer in the neural network.\n","  - `gamma`: The discount factor for future rewards in the Q-learning update.\n","  - `loss_fn`: The loss function used for training the Q-network. Default is the Huber loss (F.smooth_l1_loss).\n","  - `optim`: The optimizer used for updating the Q-network weights. Default is AdamW.\n","  - `eps_start`: The initial epsilon value for epsilon-greedy exploration.\n","  - `eps_end`: The minimum epsilon value for epsilon-greedy exploration.\n","  - `eps_last_episode`: The episode at which epsilon should reach its minimum value.\n","  - `samples_per_epoch`: The number of samples to collect in the replay buffer before starting training.\n","  - `sync_rate`: The frequency at which the `target Q-network` is synchronized with the `Q-network`.\n","  \"\"\"\n","\n","  def __init__(self, env_name, policy=epsilon_greedy, sample_fn=play_episode,\n","               capacity=100_000, batch_size=256, lr=1e-3, hidden_size=128, gamma=0.99,\n","               loss_fn=F.smooth_l1_loss, optim=AdamW, eps_start=1.0, eps_end=0.15,\n","               eps_last_episode=100, samples_per_epoch=10_000, sync_rate=10):\n","\n","    super().__init__()\n","    self.env = create_environment(env_name) # create the envireoment\n","\n","    obs_size = self.env.observation_space.shape[0] # Input shape (for DQN): state size\n","    n_actions = self.env.action_space.n # Output shape (for DQN): action size\n","\n","    ## DQN -> the policy.\n","    # get as input: state\n","    # return as output: Q-value for each action.\n","    # - The output of the DQN is a vector of Q-values, where each element\n","    #   corresponds to the estimated Q-value for a specific action.\n","    # - The Q-values represent the expected cumulative reward for taking each\n","    #   action from the current state.\n","    self.q_net = DQN(hidden_size, obs_size, n_actions).to(device)\n","\n","    ## Target DQN -> for improve the stability and convergence of the learning process\n","    # - The target Q-network helps stabilize training by providing a fixed target for a certain number of iterations.\n","    self.target_q_net = copy.deepcopy(self.q_net)\n","\n","    # define the policy function (the epsilon greedy).\n","    # the epsolin greedy balance the:\n","    # - exploration (take the best action based on the policy)\n","    # - explotation (take the random action)\n","    self.policy = policy\n","\n","    # the mamory (contains the experience)\n","    self.buffer = ReplayBuffer(capacity=capacity)\n","\n","    # save the hyperparameter (for convince)\n","    self.save_hyperparameters()\n","\n","    # ensures that the replay buffer is filled with enough samples before training begins.\n","    # The loop continues until the number of samples in the buffer reaches.\n","    while len(self.buffer) < self.hparams.samples_per_epoch:\n","      print(f\"{len(self.buffer)} samples in experience buffer. Filling...\")\n","      # sample_fn() => generate new samples using the exploration strategy\n","      #                defined by epsilon (exploration probability)\n","      # so its generate samples and push the samples to the buffer (memory)\n","      self.hparams.sample_fn(\n","          self.env, # the environment\n","          self.q_net, # the policy\n","          self.buffer, # the dataset\n","          epsilon=self.hparams.eps_start # the epcilon (for the policy that'll balance exploration & explotation)\n","        )\n","\n","  def forward(self, x):\n","    \"\"\"\n","    Forward propegation, return Q-value for each action\n","    \"\"\"\n","    output = self.q_net(x)\n","    return output\n","\n","  def configure_optimizers(self):\n","    \"\"\"\n","    Configure the optimizer for updating the Q-network weigths\n","    \"\"\"\n","    q_net_optimizer = self.hparams.optim(self.q_net.parameters(), lr=self.hparams.lr)\n","    return [q_net_optimizer]\n","\n","  def train_dataloader(self):\n","    \"\"\"\n","    Create Dataloader object for training\n","    \"\"\"\n","    # Create the dataset\n","    dataset = RLDataset(self.buffer, self.hparams.samples_per_epoch)\n","    # Create the data loader\n","    dataloader = DataLoader(\n","        dataset=dataset,\n","        batch_size=self.hparams.batch_size,\n","    )\n","    return dataloader\n","\n","  def training_step(self, batch, batch_idx):\n","    \"\"\"\n","    Execute a training step.\n","\n","    Parameters:\n","    -----------\n","    - `batch`: the batch that contaon the components of the experience.\n","    \"\"\"\n","    # 1. Extract the components from the batch (extract batch from the memory)\n","    # this is represent the transition that the agent make in the past.\n","    states, actions, rewards, dones, next_states = batch\n","    # 2. Formatting Actions, Rewards, and Dones:\n","    # reshape the tensors to match the expected format for computations.\n","    actions = actions.unsqueeze(1)\n","    rewards = rewards.unsqueeze(1)\n","    dones = dones.unsqueeze(1)\n","\n","    # 3. Q-Value Estimation:\n","    # compute the Q-value of the actions that taken (as part from the experiments) in each of the state.\n","    # - Uses the gather operation to select the Q-values corresponding to the taken actions.\n","    state_action_values = self.q_net(states).gather(1, actions)\n","\n","    # 4. Target Q-Value Estimation:\n","    # this is the Target DQN that stay stable, and not change during the training.\n","    # the Target-DQN updated each `sync_rate` epicodes.\n","    # so the Target-DQN use for calculate the target vakue for compare it to the\n","    # Q-value prediction of the DQN that updates itself each step.\n","    # - the different between those policies help us measure if the\n","    #   updated policy (that update intelf each step), better than the\n","    #   \"constant\" policy (that update intelf `sync_rate` each epicode only.)\n","    with torch.no_grad():\n","      # compute the target Q-values and select the max Q-value for each next state.\n","      next_state_values, _ = self.target_q_net(next_states).max(dim=1, keepdim=True)\n","      # Sets the target Q-values to 0 for states where the episode is done (dones).\n","      next_state_values[dones] = 0.0\n","\n","    # 5. Bellman Equation and Loss Calculation:\n","    # - Applies the Bellman equation to calculate the expected state-action values\n","    #   using the rewards and discounted future Q-values.\n","    expected_state_action_values = rewards + self.hparams.gamma * next_state_values\n","    # Compute the loss for updating the DQN weights.\n","    # - comparing the estimated Q-values and the expected Q-values.\n","    loss = self.hparams.loss_fn(state_action_values, expected_state_action_values)\n","\n","    # log the loss\n","    self.log('episode/MSE Loss', loss, on_step=False, on_epoch=True)\n","\n","    # return the loss:\n","    # - A measure of the disparity between the Q-values predicted by the\n","    #   Q-network and the Target-Q-values that derived from the Bellman equation.\n","    return loss\n","\n","\n","  def training_epoch_end(self, training_step_outputs):\n","    \"\"\"\n","    A callback function when the training epoch is end.\n","    \"\"\"\n","    # 1. Get the maximum epsilon\n","    # the idea behind is each epoch we increase the explotation,\n","    # and decease the exploration.\n","    # over the time, we want that the agent more exploit and less explor..\n","    epsilon = max(\n","        self.hparams.eps_end,\n","        self.hparams.eps_start - self.current_epoch / self.hparams.eps_last_episode\n","    )\n","\n","    # 2. update the memory with the new policy.\n","    # after each epoch, the policy is updating.\n","    # so now, we want generate more samples based on the new policy,\n","    # for improve the memory and the experience.\n","    self.hparams.sample_fn(self.env,   # environment\n","                           self.q_net, # the policy\n","                           self.buffer,# the dataset\n","                           policy=self.policy, # the policy function (epsilo greedy)\n","                           epsilon=epsilon)    # the epsilon that'll care to balance the exploration & explotation\n","\n","    # log the return of the epicode.\n","    self.log(\"episode/Return\", self.env.return_queue[-1])\n","\n","    # set in `hp_metric` folder the mean return each 200 epoch\n","    # (its for optuna optimization!)\n","    if self.current_epoch == 199:\n","      returns = list(self.env.return_queue)\n","      self.log(\"hp_metric\", statistics.mean(returns))\n","\n","    ## Updating the target-DQN weigths by the DQN weigths each `sync_rate` times\n","    if self.current_epoch % self.hparams.sync_rate == 0:\n","      self.target_q_net.load_state_dict(self.q_net.state_dict())"]},{"cell_type":"markdown","metadata":{"id":"iv-ZRdyf4mCt"},"source":["#### Create the objective function"]},{"cell_type":"code","execution_count":63,"metadata":{"id":"3B9ZoVV-QHrK","executionInfo":{"status":"ok","timestamp":1702018060332,"user_tz":-120,"elapsed":18,"user":{"displayName":"אור מזרחי","userId":"14772102086814831247"}}},"outputs":[],"source":["def objective(trial):\n","  \"\"\"\n","  An objective function to minimize (by optuna)\n","  for find the best hyperparameters of our DQN.\n","  \"\"\"\n","  # 1. With a given ramge, from each hyperparameter\n","  #    select an hyperparameter that we want to trail.\n","  lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n","  gamma = trial.suggest_float(\"gamma\", 0.0, 1.0)\n","  hidden_size = trial.suggest_categorical(\"hidden_size\", [32, 64, 128, 256])\n","  eps_end = trial.suggest_float(\"eps_end\", 0.0, 0.3)\n","\n","  # 2. Create an instance of our Deep Q-learning algorithm with\n","  # the suggested hyperparameters.\n","  algo = DeepQLearning(\n","      'LunarLander-v2',\n","      lr=lr,\n","      gamma=gamma,\n","      hidden_size=hidden_size,\n","      eps_end=eps_end\n","  )\n","\n","  # callback function after the epoch is complate.\n","  # - This callback is used for pruning, which is a technique to\n","  #   stop unpromising trials early and save computational resources.\n","  callback = PyTorchLightningPruningCallback(trial, monitor=\"episode/Return\")\n","\n","  trainer = Trainer(\n","      gpus=num_gpus,\n","      max_epochs=200,\n","      track_grad_norm=2,\n","      callbacks=[callback]\n","  )\n","  # define dictionary of the suggested hyperparameter\n","  hyperparameters = dict(\n","      lr=lr,\n","      gamma=gamma,\n","      hidden_size=hidden_size,\n","      eps_end=eps_end\n","  )\n","  # log the hyperparameters\n","  trainer.logger.log_hyperparams(hyperparameters)\n","\n","  # fit the trainer\n","  trainer.fit(algo)\n","\n","  return trainer.callback_metrics[\"episode/Return\"].item()"]},{"cell_type":"markdown","metadata":{"id":"mJUxWPhF47mW"},"source":["#### Create the optimization study"]},{"cell_type":"code","execution_count":64,"metadata":{"id":"1MynMxzZQHtz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702018060332,"user_tz":-120,"elapsed":17,"user":{"displayName":"אור מזרחי","userId":"14772102086814831247"}},"outputId":"c7dadb4d-4d27-4997-969f-4964846f7696"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2023-12-08 06:47:39,705]\u001b[0m A new study created in memory with name: no-name-0a67e0f6-c498-4d91-8552-e1df319cb438\u001b[0m\n"]}],"source":["# create the pruner (for stop the trails that not promising)\n","pruner = optuna.pruners.SuccessiveHalvingPruner()\n","# create the study\n","study = optuna.create_study(direction=\"maximize\", # maximze the value that we monitoring in the objective function. (episode/Return)\n","                            pruner=pruner)"]},{"cell_type":"markdown","metadata":{"id":"6mm9P0sX1wAA"},"source":["#### Purge logs and run the visualization tool (Tensorboard)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MfGQdpn0nY99"},"outputs":[],"source":["# Start tensorboard.\n","!rm -r /content/lightning_logs/\n","!rm -r /content/videos/\n","%load_ext tensorboard\n","%tensorboard --logdir /content/lightning_logs/"]},{"cell_type":"markdown","metadata":{"id":"9v1eHYSJ5EpO"},"source":["#### Run the hyperparameter search"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ig8c_RM8nZLN"},"outputs":[],"source":["study.optimize(objective, n_trials=20)"]},{"cell_type":"markdown","metadata":{"id":"oG0rQKyT5H-P"},"source":["#### Select and use the best hyperparameters"]},{"cell_type":"code","execution_count":67,"metadata":{"id":"ZiCp91CbPmrn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702025279675,"user_tz":-120,"elapsed":28,"user":{"displayName":"אור מזרחי","userId":"14772102086814831247"}},"outputId":"843480a6-4a92-47c5-de10-7a43049613cc"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"execute_result","data":{"text/plain":["{'lr': 0.004077209276002187,\n"," 'gamma': 0.21771064621450942,\n"," 'hidden_size': 32,\n"," 'eps_end': 0.22327832664379624}"]},"metadata":{},"execution_count":67}],"source":["study.best_params"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VyXUmpt05WaK"},"outputs":[],"source":["# Start tensorboard.\n","!rm -r /content/lightning_logs/\n","!rm -r /content/videos/\n","%load_ext tensorboard\n","%tensorboard --logdir /content/lightning_logs/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fq8UhUU0QUP-"},"outputs":[],"source":["# Now lets train the DQN with the best hyperparameters from our study\n","algo = DeepQLearning('LunarLander-v2', **study.best_params)\n","\n","trainer = Trainer(\n","  gpus=num_gpus,\n","  max_epochs=1000,\n","  track_grad_norm=2,\n",")\n","\n","trainer.fit(algo)"]},{"cell_type":"markdown","metadata":{"id":"jD3x39w71xWR"},"source":["#### Check the resulting policy"]},{"cell_type":"code","execution_count":84,"metadata":{"id":"TSElgv7I4fE6","executionInfo":{"status":"ok","timestamp":1702030768168,"user_tz":-120,"elapsed":353,"user":{"displayName":"אור מזרחי","userId":"14772102086814831247"}}},"outputs":[],"source":["display_video(episode=800)"]},{"cell_type":"code","source":[],"metadata":{"id":"VNzHySJFJiS1","executionInfo":{"status":"ok","timestamp":1702026803488,"user_tz":-120,"elapsed":13,"user":{"displayName":"אור מזרחי","userId":"14772102086814831247"}}},"execution_count":70,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"}},"nbformat":4,"nbformat_minor":0}